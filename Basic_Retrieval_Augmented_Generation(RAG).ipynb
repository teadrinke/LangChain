{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxo5dNHGM8mDLij7ULgq85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teadrinke/LangChain/blob/main/Basic_Retrieval_Augmented_Generation(RAG).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfRDc4m98uC2",
        "outputId": "822f1395-5622-4fab-8366-663cd007dcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ahsLjR9_F7R",
        "outputId": "3da98edc-3261-4bcf-fc19-8a3321453d42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/250.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Got the LLM"
      ],
      "metadata": {
        "id": "bC_29wlRBrG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"COHERE_API_KEY\"):\n",
        "  os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for COHERE:\")\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm = ChatCohere(model = \"command\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIHUkIEU_Ou4",
        "outputId": "78ce9fe8-49a1-46ed-e716-6cafa2ed57d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for COHERE:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Got the embeddings model"
      ],
      "metadata": {
        "id": "RaR-Jc0DBwTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")"
      ],
      "metadata": {
        "id": "8tOZSFtXAJk9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Got the Vector Database(in memory)"
      ],
      "metadata": {
        "id": "oLf3xlWUB1nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "7n3GBWdzAU4P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6MN2LvhEaUE",
        "outputId": "75b619b2-1b2d-44e7-90d3-24826d362f8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This is one full code for RAG"
      ],
      "metadata": {
        "id": "QKy5q1qnHidm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "# Define prompt for question-answering\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "\n",
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Define application steps\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLzWK3UoBorO",
        "outputId": "4c749f85-8198-4ab2-accf-912eae702e49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Kilha8C9ky",
        "outputId": "0bf2c267-db70-496d-97f4-28c0dc29114a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task Decomposition is a process that transforms a complex task into multiple smaller and more manageable tasks that shed light on the inner thinking process of the model. This can be done through LLMs with simple prompting, by using task-specific instructions, or with human inputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#From the below cells it is splitted"
      ],
      "metadata": {
        "id": "OZe3Xu-DHrgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading documents"
      ],
      "metadata": {
        "id": "iLdQeP1-Hy0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs4_strainer = bs4.SoupStrainer(class_ = (\"post-title\", \"post-header\", \"post-content\"))\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = (\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs = {\"parse_only\": bs4_strainer},\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "qXEIEZzyHnMr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(docs) == 1\n",
        "print(f\"Total characters : {len(docs[0].page_content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqgUjvqtZzPf",
        "outputId": "9b0c9b80-cd63-45f1-aab1-e23a3509ad9c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters : 43131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xkN3tZHZwNE",
        "outputId": "d40f4d1c-2067-4512-f0ab-aa2b5a5c70d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the documents"
      ],
      "metadata": {
        "id": "W3ZnCQ6Kej87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200, #chunk overlap is there for maintaining context\n",
        "    add_start_index = True, #track index in original document\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRDFmOCuaIFc",
        "outputId": "a409d871-9732-414b-fd9b-4387a7844f53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split blog post into 66 sub-documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Indexing the chunks"
      ],
      "metadata": {
        "id": "LQiRzwA9es7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_ids = vector_store.add_documents(documents = all_splits)\n",
        "\n",
        "print(document_ids[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjRX4KMHebAX",
        "outputId": "ad760c28-1020-4dd5-dbff-81aa3bab62f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ec7cc312-f399-4fe7-9783-3332506213c5', '70237511-9a2b-4c1d-8ac2-6cf703555c3d', '0b6a4a08-b2ec-43cd-987b-3013b1a09236']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval and Generation"
      ],
      "metadata": {
        "id": "CPoyfa4KfwHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "      {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
        ").to_messages()\n",
        "\n",
        "assert len(example_messages) == 1\n",
        "print(example_messages[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGS2hZEafDXe",
        "outputId": "cd6fdc96-f53e-4540-a487-e116a18a60c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: (question goes here) \n",
            "Context: (context goes here) \n",
            "Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "  question : str\n",
        "  context : List[Document]\n",
        "  answer : str"
      ],
      "metadata": {
        "id": "vHeT8b-2jY68"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}"
      ],
      "metadata": {
        "id": "DS22WMuIlv4z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "pNvdBbqfl6Hb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "s2sQYgk_4-uO",
        "outputId": "63302419-b585-480e-c8f7-56889615800b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "\n",
        "print(f'Context: {result[\"context\"]}\\n\\n')\n",
        "print(f'Answer: {result[\"answer\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnzNM9t75Tc3",
        "outputId": "a05b9490-c905-4011-ae3d-4b5b78d52685"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: [Document(id='d6120869-da15-45c5-9c12-84a0cdc4174a', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='fbab8224-ede4-4a59-ab08-90be284f78a2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='c2c7a10f-6986-4580-ba2d-56bae5c8c452', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='0b6a4a08-b2ec-43cd-987b-3013b1a09236', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.')]\n",
            "\n",
            "\n",
            "Answer: Task Decomposition is a process of dividing a complex task into smaller, more manageable tasks. This can be done by using simple LLM prompting, task-specific instructions, or human inputs. \n",
            "\n",
            "Task Decomposition is a useful technique that assists models in comprehending complex tasks, allowing them to utilize more test-time computation to decompose these tasks into smaller, simpler steps. This technique transforms large tasks into more manageable tasks and offers insight into the model’s thinking process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Returning the sources"
      ],
      "metadata": {
        "id": "fgZOWFMx6CaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = '''Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:'''\n",
        "\n",
        "custom_rag_template = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "sS_WfHmx5zOd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Query analysis"
      ],
      "metadata": {
        "id": "5X7_i4vKbbdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application"
      ],
      "metadata": {
        "id": "lZWVHw5SbiIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_documents = len(all_splits)\n",
        "third = total_documents//3\n",
        "\n",
        "for i, document in enumerate(all_splits):\n",
        "  if i< third:\n",
        "    document.metadata[\"section\"] = \"beginning\"\n",
        "  elif i<2*third:\n",
        "    document.metadata[\"section\"] = \"middle\"\n",
        "  else:\n",
        "    document.metadata[\"section\"] = \"end\"\n",
        "\n",
        "all_splits[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bbY0TaXaZvA",
        "outputId": "bca42be6-aff0-4887-cb0f-8b2d9da2ded6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              " 'start_index': 8,\n",
              " 'section': 'beginning'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "_ = vector_store.add_documents(all_splits)"
      ],
      "metadata": {
        "id": "vprsrQ2ocS_i"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "class Search(TypedDict):\n",
        "  \"\"\"Search Query.\"\"\"\n",
        "\n",
        "  query : Annotated[str, ... , \"Search query to run.\"]\n",
        "  section : Annotated[\n",
        "      Literal[\"beginning\", \"middle\", \"end\"],\n",
        "      ...,\n",
        "      \"Section to query.\",\n",
        "  ]"
      ],
      "metadata": {
        "id": "-2fOn58EctqO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "  question : str\n",
        "  query : Search\n",
        "  context : List[Document]\n",
        "  answer : str\n",
        "\n",
        "def analyze_query(state : State):\n",
        "  structured_llm = llm.with_structured_output(Search)\n",
        "  query = structured_llm.invoke(state[\"question\"])\n",
        "  return {\"query\" : query}\n",
        "\n",
        "def retrieve(state : State):\n",
        "  query = state[\"query\"]\n",
        "  retrieved_docs = vector_store.similarity_search(\n",
        "      query = [\"query\"],\n",
        "      filter = lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
        "  )\n",
        "\n",
        "  return {\"context\" : retrieved_docs}\n",
        "\n",
        "def generate(state : State):\n",
        "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "  messages = prompt.invoke({\"question\" : state[\"question\"], \"context\" : docs_content})\n",
        "  response = llm.invoke(messages)\n",
        "  return {\"answer\" : response.content}\n",
        "\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
        "graph_builder.add_edge(START, \"analyze_query\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "sqhWkbmjfZi8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "dcV0OP5Cimn3",
        "outputId": "b39efd44-cdab-44d2-b905-96081e84c8f9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for step in graph.stream(\n",
        "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
        "    stream_mode=\"updates\",\n",
        "):\n",
        "    print(f\"{step}\\n\\n----------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "gSeynocTjaIk",
        "outputId": "5e7e3122-73db-43c2-c0d8-c1292d1c0c3b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "status_code: 400, body: {'message': 'invalid request: tool use is not supported by the provided model: command.'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-66b5ad5d9aa2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m for step in graph.stream(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What does the end of the post say about Task Decomposition?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"updates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ):\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{step}\\n\\n----------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1657\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 )\n\u001b[1;32m    407\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-21c7e9c0ec9c>\u001b[0m in \u001b[0;36manalyze_query\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mstructured_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_structured_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSearch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructured_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5352\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5353\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5354\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5355\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5356\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_cohere/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_generation_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m\"To suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_async_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcheck_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_async_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/base_client.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, message, accepts, model, preamble, chat_history, conversation_id, prompt_truncation, connectors, search_queries_only, documents, citation_quality, temperature, max_tokens, max_input_tokens, k, p, seed, stop_sequences, frequency_penalty, presence_penalty, raw_prompting, return_prompt, tools, tool_results, force_single_step, response_format, safety_mode, request_options)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 )\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m                 raise BadRequestError(\n\u001b[0m\u001b[1;32m   1066\u001b[0m                     typing.cast(\n\u001b[1;32m   1067\u001b[0m                         \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: status_code: 400, body: {'message': 'invalid request: tool use is not supported by the provided model: command.'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDqgWS_5kCk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}