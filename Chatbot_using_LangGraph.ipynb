{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2Ttnk8jCaobqYarf/AGpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teadrinke/LangChain/blob/main/Chatbot_using_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTPeezbxRpq_"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core langgraph>0.2.27"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh8qsRm6UTti",
        "outputId": "7e21ddd6-408e-43a6-c4d0-d042d826084e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/249.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langgraph"
      ],
      "metadata": {
        "id": "h3XjjRjFgX6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip langgraph --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKjoXw09g8go",
        "outputId": "e0f04341-97dc-442d-fb64-7e4200257c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"langgraph\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(langgraph.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Wr5f8jyRgTIB",
        "outputId": "0da42573-49fd-4b2a-e5af-7a03116ce91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'langgraph' has no attribute '__version__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-40de123d40ea>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'langgraph' has no attribute '__version__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"COHERE_API_KEY\"):\n",
        "  os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere\")\n",
        "\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "model = ChatCohere(model = \"command\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0TZXtlwThY6",
        "outputId": "b9ac53c1-6e12-49cc-e31e-be45fd4a00b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Cohere··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "model.invoke([HumanMessage(content = \"Hi, I'm Bob\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sAMBy43U_aI",
        "outputId": "8cae2ff7-4944-4de7-df35-19a3d5864020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi Bob! I'm Coral, very nice to meet you. If you have any questions, let me know and I'll do my best to help you out.\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'e4ef7ebf-1d24-4590-adcc-a691cbaabfff', 'token_count': {'input_tokens': 67.0, 'output_tokens': 34.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'e4ef7ebf-1d24-4590-adcc-a691cbaabfff', 'token_count': {'input_tokens': 67.0, 'output_tokens': 34.0}}, id='run-8f163de9-7528-4743-8469-548837779bcd-0', usage_metadata={'input_tokens': 67, 'output_tokens': 34, 'total_tokens': 101})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke([HumanMessage(content = \"What's my name?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdPcj4sRW0KK",
        "outputId": "fa9d4802-7649-42f3-9ad6-3d0ca3f0c8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I don't know your name as we haven't been introduced yet. I'd love to know yours, so we can start on a more formal note :) \\n\\nCan you share your name with me? No worries if you'd prefer to stay anonymous, feel free to let me know if I can assist you with anything else instead!\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '136ea558-78c5-4510-9554-59fa88aaac0c', 'token_count': {'input_tokens': 67.0, 'output_tokens': 68.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '136ea558-78c5-4510-9554-59fa88aaac0c', 'token_count': {'input_tokens': 67.0, 'output_tokens': 68.0}}, id='run-8d1df649-e711-4924-8cbc-add758628fdc-0', usage_metadata={'input_tokens': 67, 'output_tokens': 68, 'total_tokens': 135})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To maintain the context you have to pass the entire convo history to the model"
      ],
      "metadata": {
        "id": "FHBYeCsdZrTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content = \"Hi, I am Bob\"),\n",
        "        AIMessage(content = \"Hello Bob, How can I assist you today?\"),\n",
        "        HumanMessage(content = \"What is my name?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlecpPccXCtJ",
        "outputId": "1d41984a-9549-4c3f-8946-fa24d2850b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your name is Bob. Isn't that right?\", additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '5ecb733c-9bb0-4ef7-b945-7ec4d07efdd9', 'token_count': {'input_tokens': 89.0, 'output_tokens': 11.0}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '5ecb733c-9bb0-4ef7-b945-7ec4d07efdd9', 'token_count': {'input_tokens': 89.0, 'output_tokens': 11.0}}, id='run-cd04d1ea-f477-4047-989e-265fba8b24c0-0', usage_metadata={'input_tokens': 89, 'output_tokens': 11, 'total_tokens': 100})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MessageState"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH-AVMCTfjxx",
        "outputId": "243d33b3-9762-4b5e-a0a3-1c5141e413bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement MessageState (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for MessageState\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PnYl-yOfpvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "# from langgraph.graph.new_location import MessageState\n",
        "\n",
        "workflow = StateGraph(state_schema = MessagesState)\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "  response = model.invoke(state[\"messages\"])\n",
        "  return {\"messages\" : response}\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer = memory)"
      ],
      "metadata": {
        "id": "l-tCVrloYGnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\" : {\"thread_id\":\"abc123\"}}"
      ],
      "metadata": {
        "id": "JPYCN9A9fa85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi, I am Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\" : input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg_1xWiHjtYw",
        "outputId": "805e7525-18d0-4a9f-8772-0c4202a91388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Nice to meet you, Bob! I'm Coral, a brilliant, sophisticated, AI-assistant chatbot trained to assist human users by providing thorough responses. If you have any questions, please let me know, and I'll do my best to help you out!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\" : input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxtDQNYAkXjV",
        "outputId": "afefa3a9-ecfc-49c3-d2ba-6e85d0ed6d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob! But, would you like to know some fun facts about the name Bob? \n",
            "\n",
            "Or, would you like me to answer a different question entirely? Let's discuss further about your preferences and I can attempt to help with any information you may require!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You talk like a pirate. Answer all questions to best of your abilities.\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "AigOV369kxjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema = MessagesState)\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "  prompt = prompt_template.invoke(state)\n",
        "  response = model.invoke(prompt)\n",
        "  return {\"messages\" : response}\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer = memory)"
      ],
      "metadata": {
        "id": "RGY6DF2DGPc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\" : {\"thread_id\" : \"abc345\"}}\n",
        "query = \"Hi, I am Jim.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\" : input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYt3xgGmJgRZ",
        "outputId": "6c1fb863-a2a0-4e3c-da1a-4f01c9edd0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Arrr, Jim! I be Coral, yer pirate AI-assistant chatbot, and I be trained to answer yer questions and help in any way I can. What be yer first task for ye olde Coral?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEhbDCtcK3Tg",
        "outputId": "fb9db433-0896-4893-c4be-cc393b84bbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ah, Jim, yer a pirate through and through, always wantin' to know things. \n",
            "\n",
            "And, yessir, yer name be Jim! Or, to translate to pirate-speak, it be Jim the Swabby. \n",
            "\n",
            "What else can I do for you, Jim?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are helpful assistant. Answer all questions to the best of your ability in {language}\"\n",
        "\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ypJLrBJILwD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "  messages : Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "  language: str\n",
        "\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "\n",
        "def call_model(state: State):\n",
        "    prompt = prompt_template.invoke(state)\n",
        "    response = model.invoke(prompt)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "wJTPfy1gNHaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\" : {\"thread_id\" : \"abc456\"}}\n",
        "query = \"Hi, I'm Bob\"\n",
        "language = \"Spanish\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IztSCFUeP8sc",
        "outputId": "58acbb5e-2255-4078-a172-95ae62753e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "¡Hola Bob! Me llamo Coral, y me encanta ayudarte. Que tengas un excelente día. \n",
            "\n",
            "If you have any questions, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the entire state is persisted, so we can omit parameters like language if no changes are desired"
      ],
      "metadata": {
        "id": "FJIvpeykQpOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx5HPAk4QWYE",
        "outputId": "550269fc-723f-4dcd-ebba-871d64f9cd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You introduced yourself as \"Bob\".\n",
            "\n",
            "¿Qué tal es \"Bob\" como se escribe en español?: \"Bób\"? \n",
            "\n",
            "Can I help you with anything else?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot send full list of message history to the chatbot hence we have to trim or limit our messages history size"
      ],
      "metadata": {
        "id": "I-gZm9SBLCkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens = 65,\n",
        "    strategy = \"last\",\n",
        "    token_counter = model,\n",
        "    include_system = True,\n",
        "    allow_partial = False,\n",
        "    start_on = \"human\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]\n",
        "\n",
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxO-5G9JQgbF",
        "outputId": "e0bbf308-ada8-487e-dd7f-feeb9ad94bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "\n",
        "def call_model(state: State):\n",
        "  trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "  prompt = prompt_template.invoke(\n",
        "      {\"messages\" : trimmed_messages, \"language\" : state[\"language\"]}\n",
        "  )\n",
        "  response = model.invoke(prompt)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "bqJ1DXaBL5be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
        "query = \"What is my name?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yE9oWf6NO_i",
        "outputId": "567dd6c0-d4cc-49ea-a3a0-5ac282c9b40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob. Would you like me to ask you questions about Bob, or would you like to talk about something else?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
        "query = \"What math problem did I ask?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y12pcSuwNTai",
        "outputId": "803b5a59-6945-44eb-fc93-0a6657237d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You asked for the answer to 2 + 2, which I answered as 4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\" : {\"thread_id\" : \"abc789\"}}\n",
        "query = \"Hi I'm Todd, please tell me a joke.\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "for chunk, metadata in app.stream(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        "    stream_mode=\"messages\",\n",
        "):\n",
        "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
        "        print(chunk.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GTojB0rNfmq",
        "outputId": "51aff845-1b0a-4488-930c-3e06ff2fb4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure|,| Todd|!| Here|'s| a| joke| for| you|:|\n",
            "\n",
            "Why| was| the| computer| cold|?|\n",
            "\n",
            "Because| it| left| its| Windows| open|!|\n",
            "\n",
            "Hope| you| found| that| joke| to| be| as| cool| as| a| computer|!| If| you|'d| like|,| I| can| tell| you| another| one|.||"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0NEt21yOO6PU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}